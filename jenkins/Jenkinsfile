pipeline {
  agent {
    docker {
      image 'bogdandragos/jenkins-k8s-libvirt-agent:latest'
      args """
        --user 0
        --privileged
        --network host
        -v /var/run/libvirt/libvirt-sock:/var/run/libvirt/libvirt-sock
        -v /run/libvirt:/run/libvirt
        -v /var/lib/libvirt/images:/var/lib/libvirt/images
        -v /root/.ssh:/root/.ssh:ro
      """
      reuseNode true
    }
  }

  parameters {
    booleanParam(name: 'SKIP_TERRAFORM', defaultValue: true, description: 'Skip Terraform stages')
    booleanParam(name: 'SKIP_ANSIBLE', defaultValue: true, description: 'Skip Ansible stages') 
    booleanParam(name: 'INSTALL_METALLB', defaultValue: true, description: 'Install MetalLB')
    booleanParam(name: 'INSTALL_INGRESS', defaultValue: true, description: 'Install Nginx Ingress Controller with MetalLB integration')
  }

  options {
    ansiColor('xterm')
    timestamps()
    timeout(time: 90, unit: 'MINUTES')
    buildDiscarder(logRotator(numToKeepStr: '20'))
  }

  environment {
    LIBVIRT_URI         = 'qemu:///system'
    ANSIBLE_INVENTORY   = 'inventory.ini'
    KUBE_SSH_PUBLIC_KEY = credentials('kube-ssh-public-key')
    ANSIBLE_HOST_KEY_CHECKING = 'False'
    ANSIBLE_STDOUT_CALLBACK = 'yaml'
  }

  stages {
    stage('Prepare Base Image') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        sh '''
          echo "\\033[36m[Prepare Base Image]\\033[0m"
          BASE_IMAGE_PATH="/var/lib/libvirt/images/ubuntu-24.04-server-cloudimg-amd64.img"
          BASE_IMAGE_URL="https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img"
          if [ ! -f "$BASE_IMAGE_PATH" ]; then
            echo "Downloading base image..."
            wget "$BASE_IMAGE_URL" -O "$BASE_IMAGE_PATH" --progress=dot:giga
          else
            echo "Base image already exists - skipping download."
          fi
          chmod 666 "$BASE_IMAGE_PATH" || true
          chown 107:107 "$BASE_IMAGE_PATH" || true
        '''
      }
    }

    stage('Setup Libvirt Pool') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        sh '''
          echo "\\033[36m[Setup Libvirt Pool]\\033[0m"
          virsh pool-define-as default dir --target /var/lib/libvirt/images || true
          virsh pool-start default || true
          virsh pool-autostart default || true
        '''
      }
    }

    stage('Checkout Code') {
      steps {
        git url: 'git@github.com:bogdandragosvasile/k8s-libvirt-cluster.git',
            branch: 'master',
            credentialsId: 'github-ssh-key'
      }
    }

    stage('Terraform Init') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        dir('terraform') {
          sh 'terraform init'
        }
      }
    }

    stage('Terraform Plan') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        dir('terraform') {
          sh '''
            terraform plan \\
              -var="kube_ssh_public_key=${KUBE_SSH_PUBLIC_KEY}" \\
              -out=plan.tfout
          '''
        }
      }
    }

    stage('Terraform Apply') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        dir('terraform') {
          sh 'terraform apply -auto-approve plan.tfout'
        }
      }
    }

    stage('Generate Ansible Inventory') {
      when { expression { !params.SKIP_TERRAFORM } }
      steps {
        dir('terraform') {
          sh 'terraform output -json > outputs.json && cat outputs.json'
        }
        sh '''
          echo "\\033[36m[Preserve original vars.yaml and append Terraform outputs]\\033[0m"
          
          # Show original vars.yaml
          echo "Original vars.yaml:"
          cat ansible/vars.yaml
          
          # Append Terraform outputs to existing vars.yaml (don't overwrite!)
          echo "" >> ansible/vars.yaml
          echo "# Terraform VM outputs:" >> ansible/vars.yaml
          jq -r 'to_entries | map("\\(.key): \\(.value.value)") | join("\\n")' terraform/outputs.json >> ansible/vars.yaml
          
          echo "\\033[36m[Final vars.yaml with Terraform outputs:]\\033[0m"
          cat ansible/vars.yaml
          
          echo "\\033[36m[Check inventory.ini]\\033[0m"
          ls -l ansible/inventory.ini
          cat ansible/inventory.ini
        '''
      }
    }

    stage('Wait for VMs to be Reachable via SSH') {
      when { expression { !params.SKIP_ANSIBLE } }
      steps {
        sh '''
          echo "\\033[36m[Wait for VMs over SSH]\\033[0m"
          MAX_RETRIES=60
          SLEEP_TIME=10
          cd terraform

          for host in $(jq -r '.vm_ips.value | to_entries | map(.value) | .[]' outputs.json); do
            echo "Checking SSH access on $host..."
            for i in $(seq 1 $MAX_RETRIES); do
              if ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\
                -i /root/.ssh/id_ed25519 -o ConnectTimeout=3 ubuntu@$host 'true' 2>/dev/null; then
                echo "OK: $host reachable via SSH"
                break
              fi
              echo "Waiting for $host... ($i/$MAX_RETRIES)"
              sleep $SLEEP_TIME
            done
            if [ $i -eq $MAX_RETRIES ]; then
              echo "\\033[31mERROR: $host not reachable via SSH\\033[0m"
              exit 1
            fi
          done
        '''
      }
    }

    stage('Run Ansible Playbook') {
      when { expression { !params.SKIP_ANSIBLE } }
      steps {
        dir('ansible') {
          script {
            try {
              sh '''
                echo "\\033[36m[Ansible Setup - Enhanced Error Detection]\\033[0m"
                
                # Set strict error handling
                set -euo pipefail
                
                # Run playbook with enhanced verbose output and error detection
                ANSIBLE_HOST_KEY_CHECKING=False \\
                ANSIBLE_STDOUT_CALLBACK=yaml \\
                ansible-playbook -i ${ANSIBLE_INVENTORY} main.yaml \\
                  --extra-vars "@vars.yaml" \\
                  -vvv \\
                  --diff
                
                # Verify the deployment was successful
                echo "\\033[36m[Post-Deployment Verification]\\033[0m"
                ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
                  -a "kubectl get nodes" \\
                  --extra-vars "@vars.yaml" || true
              '''
            } catch (Exception e) {
              echo "\\033[31m[ANSIBLE PLAYBOOK FAILED]\\033[0m"
              echo "Error: ${e.getMessage()}"
              
              // Collect diagnostic information
              sh '''
                echo "\\033[33m[Collecting Diagnostic Information]\\033[0m"
                
                # Check if any nodes are accessible
                ansible -i ${ANSIBLE_INVENTORY} all -m ping --extra-vars "@vars.yaml" || true
                
                # Check kubelet status on all nodes
                echo "\\033[33m[Checking kubelet status]\\033[0m"
                ansible -i ${ANSIBLE_INVENTORY} all -m shell \\
                  -a "sudo systemctl status kubelet" \\
                  --extra-vars "@vars.yaml" || true
                
                # Check if control plane is working
                echo "\\033[33m[Checking control plane status]\\033[0m"
                ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
                  -a "kubectl get nodes" \\
                  --extra-vars "@vars.yaml" || true
              '''
              
              // Re-throw the exception to fail the build
              throw e
            }
          }
        }
      }
    }

    stage('Verify Cluster Status') {
      when { expression { !params.SKIP_ANSIBLE } }
      steps {
        dir('ansible') {
          sh '''
            echo "\\033[36m[Final Cluster Verification]\\033[0m"
            
            # Get cluster status
            ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
              -a "kubectl get nodes -o wide" \\
              --extra-vars "@vars.yaml"
            
            # Get pod status
            ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
              -a "kubectl get pods --all-namespaces" \\
              --extra-vars "@vars.yaml"
            
            # Check cluster info
            ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
              -a "kubectl cluster-info" \\
              --extra-vars "@vars.yaml"
          '''
        }
      }
    }

    stage('Install MetalLB') {
      when { expression { params.INSTALL_METALLB } }
      steps {
        script {
          sh '''
            echo "\\033[36m[Installing and configuring MetalLB]\\033[0m"
            
            # Get control plane IP
            CONTROL_PLANE_IP=$(grep -A1 "control_plane_init" ansible/inventory.ini | grep -E "^[0-9]" | awk '{print $1}')
            echo "Control plane IP: $CONTROL_PLANE_IP"
            
            # Test SSH connection
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} "echo 'SSH connection successful'"
            
            # Copy the MetalLB installation script
            scp -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 scripts/install-metallb.sh ubuntu@${CONTROL_PLANE_IP}:/tmp/
            
            # Make it executable and run it
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} \\
              "chmod +x /tmp/install-metallb.sh && /tmp/install-metallb.sh"
            
            echo "üéâ MetalLB installation completed successfully!"
          '''
        }
      }
    }


    stage('Install Nginx Ingress Controller') {
      when { expression { params.INSTALL_INGRESS } }
      steps {
        script {
          sh '''
            echo "\\033[36m[Installing Nginx Ingress Controller]\\033[0m"
            
            # Get control plane IP
            CONTROL_PLANE_IP=$(grep -A1 "control_plane_init" ansible/inventory.ini | grep -E "^[0-9]" | awk '{print $1}')
            echo "Control plane IP: $CONTROL_PLANE_IP"
            
            # Copy the ingress installation script
            scp -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 scripts/install-nginx-ingress.sh ubuntu@${CONTROL_PLANE_IP}:/tmp/
            
            # Make it executable and run it
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} \\
              "chmod +x /tmp/install-nginx-ingress.sh && /tmp/install-nginx-ingress.sh"
            
            echo "üéâ Nginx Ingress Controller installation completed successfully!"
          '''
        }
      }
    }




    stage('Final Cluster Summary') {
      steps {
        script {
          sh '''
            echo "=================================="
            echo "üéâ KUBERNETES CLUSTER SUMMARY"
            echo "=================================="
            
            # Get control plane IP
            CONTROL_PLANE_IP=$(grep -A1 "control_plane_init" ansible/inventory.ini | grep -E "^[0-9]" | awk '{print $1}')
            
            echo "üìä CLUSTER STATUS:"
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} "kubectl get nodes -o wide"
            
            echo "üöÄ SYSTEM PODS:"
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} "kubectl get pods -n kube-system"
            
            echo "‚öñÔ∏è METALLB STATUS:"
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} "kubectl get pods -n metallb-system" || echo "MetalLB not installed"
            
            echo "üåê SERVICES WITH EXTERNAL IPs:"
            ssh -o StrictHostKeyChecking=no -i /root/.ssh/id_ed25519 ubuntu@${CONTROL_PLANE_IP} "kubectl get svc --all-namespaces -o wide | grep -E 'LoadBalancer|EXTERNAL-IP'" || echo "No LoadBalancer services found"
            
            echo "‚úÖ Your Kubernetes cluster is ready!"
          '''
        }
      }
    }
  }

  post {
    always {
      script {
        // Collect logs regardless of success/failure
        dir('ansible') {
          sh '''
            echo "\\033[36m[Collecting Final Logs]\\033[0m"
            
            # Show final cluster state
            ansible -i ${ANSIBLE_INVENTORY} control_plane_init -m shell \\
              -a "kubectl get nodes" \\
              --extra-vars "@vars.yaml" || echo "Could not get cluster status"
            
            echo "Pipeline completed at $(date)"
          '''
        }
      }
      echo 'Pipeline completed.'
    }
    
    success {
      echo '\\033[32m[SUCCESS] Kubernetes cluster deployment completed successfully!\\033[0m'
    }
    
    failure {
      script {
        echo '\\033[31m[FAILURE] Pipeline failed - collecting diagnostic information\\033[0m'
        
        // Enhanced cleanup and diagnostics on failure
        sh '''
          echo "\\033[33m[Cleanup on Failure]\\033[0m"
          
          # Optional: Uncomment to clean up VMs on failure
          # if [ -x ./scripts/cleanup_vms.sh ]; then
          #   ./scripts/cleanup_vms.sh || true
          # fi
          
          # Show what infrastructure exists
          cd terraform && terraform show || true
        '''
      }
    }
  }
}
